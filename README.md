# Next Word Prediction Using LSTM-RNN

A natural language processing project that builds an LSTM-based recurrent neural network to predict the next word in a sequence—ideal for text completion, autocompletion, and language modeling tasks.

---

##  Project Overview

This repository showcases a machine learning model developed using an **LSTM (Long Short-Term Memory)** architecture to predict the most probable next word given a sequence of words. The project demonstrates end-to-end steps, including data preprocessing, model training, and interactive prediction.

---

##  Key Features

- **LSTM-based architecture**: Able to learn and remember long-term dependencies in text.
- **Text preprocessing pipeline**: Tokenization, sequence generation, padding, and vocabulary creation.
- **Model evaluation**: Visualizations of training/validation accuracy and loss.
- **Interactive prediction**: Input custom text seeds to generate next word suggestions in real-time.

---

##  Project Structure

| File/Folder       | Description                                           |
|-------------------|-------------------------------------------------------|
| `lstm.ipynb`  | Jupyter notebook containing full code and analysis    |
| `hamlet.txt/`           | Text dataset         |
| `next_word.h5/`          |  Saved model file   |

---

##  Technologies Used

- **Python** – Core language for model development
- **TensorFlow / Keras** – Deep learning frameworks for building and training LSTM
- **NumPy** – Handling arrays and numerical data
- **Matplotlib / Seaborn** – Visualizing training performance
- **Jupyter Notebook** – Interactive coding and exploration

---
